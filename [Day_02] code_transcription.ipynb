{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Day 02] code_transcription.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Forqb31zlKSE"
      },
      "source": [
        "## **Binary classification : Image classification**\n",
        "\n",
        "     (참고) https://www.kaggle.com/devm2024/transfer-learning-with-vgg-16-cnn-aug-lb-0-1712"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moDer3HMlW2_"
      },
      "source": [
        "해당 코드는 이유한님이 정리해주신 kaggle-korea 커널 커리큘럼 중 **\"Binary classification: Image classification\"**의 첫번째 파트인 Statoil/C-CORE Iceberg Classifier Challenge 파트를 필사한 내용입니다. \n",
        "\n",
        "그 중 Transfer Learning with VGG-16 CNN+AUG LB 0.1712 글을 참고했으며 출처는 별도로 표기해두었습니다. \n",
        "\n",
        "코드를 필사하면서 개인적인 코드 해석을 추가해서 업데이트할 예정입니다. \n",
        "\n",
        "**목표: 매일 1개 커밋 업로드 및 코드 구현능력 향상**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZblKMxW05UF"
      },
      "source": [
        "### **Transfer Learning with VGG-16 CNN+AUG LB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpNt72Cy05IO"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from subprocess import check_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Laym5N1I0s6e"
      },
      "source": [
        "# 패키지 import\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from os.path import join as opj\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pylab\n",
        "plt.rcParams['figure.figsize'] = 10, 10\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvCu1rZhmN4y",
        "outputId": "fb190d21-a068-4ba8-fbfb-bda4e6487190"
      },
      "source": [
        "from google.colab import drive # Google colab과 Google drive 연동\n",
        "\n",
        "# 출력되는 url 입력시 연동 완료\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0W_1QqymRXt"
      },
      "source": [
        "# json file load\n",
        "train = pd.read_json('/content/gdrive/My Drive/dataset/ice berg/train.json')\n",
        "test = pd.read_json('/content/gdrive/My Drive/dataset/ice berg/test.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D-PNBtWmbQv"
      },
      "source": [
        "target_train = train['is_iceberg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GNWs6FAJ77Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ab8434-b9ae-4ac0-c2cc-2706dfedf59a"
      },
      "source": [
        "# is_iceberg는 binary한 값으로 테이블이 이뤄진 것을 확인할 수 있습니다 \n",
        "target_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    1\n",
              "3    0\n",
              "4    0\n",
              "Name: is_iceberg, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GbEnI78mwxZ"
      },
      "source": [
        "# errors = 'coerce' 문자형을 숫자로 변경할 때 에러를 무시하는 옵션 \n",
        "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors = 'coerce')\n",
        "train['inc_angle'] = pd.to_numeric(train['inc_angle'], errors = 'coerce')\n",
        "train['inc_angle'] = train['inc_angle'].fillna(method='pad') # \"pad\" 옵션을 통해 앞에서부터 데이터를 채워나간다 \n",
        "X_angle = train['inc_angle']\n",
        "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors = 'coerce')\n",
        "X_test_angle = test['inc_angle']\n",
        "\n",
        "# Generate the training data\n",
        "X_band_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_1']])\n",
        "X_band_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_2']])\n",
        "X_band_3 = (X_band_1 + X_band_2)/2\n",
        "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis], X_band_3[:, :, :, np.newaxis]], axis = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUD4d78bLoPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007cac6a-9170-4b8f-b811-57195f699915"
      },
      "source": [
        "X_band_1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1604, 75, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBBoD6Z-oNxa"
      },
      "source": [
        "X_band_test_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
        "X_band_test_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
        "X_band_test_3 = (X_band_test_1 + X_band_test_2)/2\n",
        "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis], X_band_test_3[:, :, :, np.newaxis]], axis = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv3jXnBdo8NQ"
      },
      "source": [
        "# import keras\n",
        "from matplotlib import pyplot\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras import initializers\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIlwggRQqgm_"
      },
      "source": [
        "# data augment for multi-input\n",
        "# shift, zoom 등의 옵션을 지정해 데이터 generator를 실시할 수 있습니다 \n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size = 64\n",
        "gen = ImageDataGenerator(horizontal_flip = True,\n",
        "                         vertical_flip = True, \n",
        "                         width_shift_range = 0.,\n",
        "                         height_shift_range = 0.,\n",
        "                         channel_shift_range = 0., \n",
        "                         zoom_range = 0.2,\n",
        "                         rotation_range = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANONfzpqyJH9"
      },
      "source": [
        "# return: 반환 즉시 함수를 마침\n",
        "# yield: 잠시 함수 바깥의 코드가 실행되도록 양보해 값을 가져가게 한 뒤 제너레이터 내의 코드 실행 \n",
        "def gen_flow_for_two_inputs(X1, X2, y): \n",
        "  genX1 = gen.flow(X1, y, batch_size = batch_size, seed = 5)\n",
        "  genX2 = gen.flow(X1, X2, batch_size = batch_size, seed = 5)\n",
        "  while True:\n",
        "    X1i = genX1.next()\n",
        "    X2i = genX2.next()\n",
        "    yield [X1i[0], X2i[1]], X1i[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvantLG9yvrT"
      },
      "source": [
        "# generator를 위한 함수 정의\n",
        "def get_callbacks(filepath, patience = 2):\n",
        "  es = EarlyStopping('val_loss', patience = 10, mode = \"min\")\n",
        "  msave = ModelCheckpoint(filepath, save_best_only=True)\n",
        "  return [es, msave]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgX3eB9zAwt"
      },
      "source": [
        "def getVggAngleModel(): \n",
        "  input_2 = Input(shape = [1], name = \"angle\")\n",
        "  angle_layer = Dense(1, )(input_2)\n",
        "  base_model = VGG16(weights = \"imagenet\", include_top = False, input_shape = X_train.shape[1:], classes = 1)\n",
        "  x = base_model.get_layer(\"block5_pool\").output\n",
        "\n",
        "# globalaveragepooling layer(GAP)는 각 feature map 상의 노드값들의 평균을 출력한다. output: (1,1,d)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  merge_one = concatenate([x, angle_layer])\n",
        "  merge_one = Dense(512, activation = \"relu\", name = \"fc2\")(merge_one)\n",
        "  merge_one = Dropout(0.3)(merge_one)\n",
        "  merge_one = Dense(512, activation = \"relu\", name = \"fc3\")(merge_one)\n",
        "  merge_one = Dropout(0.3)(merge_one)\n",
        "\n",
        "  predictions = Dense(1, activation = \"sigmoid\")(merge_one)\n",
        "\n",
        "  model = Model([base_model.input, input_2], predictions)\n",
        "  sgd = SGD(lr = 1e-3, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "  model.compile(loss = \"binary_crossentropy\", optimizer = sgd, metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t1bq7A56Vy8"
      },
      "source": [
        "# TypeError: ('Keyword argument not understood:', 'inputs') 오류 해결"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGrJ7epzity"
      },
      "source": [
        "def myAngleCV(X_train, X_angle, X_test):\n",
        "  K = 3\n",
        "  folds = list(StratifiedKFold(n_splits = K, shuffle = True, random_state = 16).split(X_train, target_train))\n",
        "  y_test_pred_log = 0\n",
        "  y_train_pred_log = 0\n",
        "  y_valid_pred_log = 0.0*target_train\n",
        "  for j, (train_idx, test_idx) in enumerate(folds):\n",
        "    print(\"\\n============FOLD====\", j)\n",
        "    X_train_cv = X_train[train_idx]\n",
        "    y_train_cv = target_train[train_idx]\n",
        "    X_holdout = X_train[test_idx]\n",
        "    Y_holdout = target_train[test_idx]\n",
        "\n",
        "    # angle\n",
        "    X_angle_cv = X_angle[train_idx]\n",
        "    X_angle_hold = X_angle[test_idx]\n",
        "\n",
        "    # 파일 경로를 정의하고 callback 값을 부른다\n",
        "    file_path = \"%s_aug_model_weights.hdf5\"%j #모델을 각 j값에 대해 저장한다 \n",
        "    callbacks = get_callbacks(filepath = file_path, patience=5)\n",
        "    gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n",
        "    galaxyModel = getVggAngleModel()\n",
        "    galaxyModel.fit_generator(\n",
        "        gen_flow, \n",
        "        steps_per_epoch = 2, \n",
        "        epochs = 2, \n",
        "        shuffle = True, \n",
        "        verbose = 1, \n",
        "        validation_data = ([X_holdout, X_angle_hold], Y_holdout), \n",
        "        callbacks = callbacks)\n",
        "    \n",
        "    galaxyModel.load_weights(filepath = file_path)\n",
        "    score = galaxyModel.evaluate([X_train_cv, X_angle_cv], y_train_cv, verbose = 0)\n",
        "    print(\"Train loss: \", score[0])\n",
        "    print(\"Test accuracy: \", score[1])\n",
        "\n",
        "    # get validation score\n",
        "    pred_valid = galaxyModel.predict([X_holdout, X_angle_hold])\n",
        "    y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
        "\n",
        "    # get test score\n",
        "    temp_test = galaxyModel.predict([X_test, X_test_angle])\n",
        "    y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
        "\n",
        "    temp_train = galaxyModel.predict([X_train, X_angle])\n",
        "    y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
        "\n",
        "  y_test_pred_log = y_test_pred_log/K\n",
        "  y_train_pred_log = y_train_pred_log/K\n",
        "  \n",
        "  print(\"\\n Train Log Logg Validation = \", log_loss(target_train, y_train_pred_log))\n",
        "  print(\" Test Log Loss Validation = \", log_loss(target_train, y_valid_pred_log))\n",
        "  return y_test_pred_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U0rYw81TdH9",
        "outputId": "494a1be5-5722-4673-8d55-dd034b1235ba"
      },
      "source": [
        "# 코드 출력값을 확인하기위해 steps_per_epoch, epochs, K 값을 일부 수정하여 성능이 낮게 나왔습니다. \n",
        "# 성능향상을 위해서는 해당 파라미터의 값을 높이는 것도 좋을 것 같습니다.  \n",
        "preds = myAngleCV(X_train, X_angle, X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "============FOLD==== 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2/2 [==============================] - 56s 41s/step - loss: 1.3166 - accuracy: 0.4948 - val_loss: 0.7215 - val_accuracy: 0.4935\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 58s 42s/step - loss: 0.8877 - accuracy: 0.5573 - val_loss: 0.6532 - val_accuracy: 0.6112\n",
            "Train loss:  0.6446225047111511\n",
            "Test accuracy:  0.6164639592170715\n",
            "\n",
            "============FOLD==== 1\n",
            "Epoch 1/2\n",
            "2/2 [==============================] - 56s 42s/step - loss: 1.0227 - accuracy: 0.5156 - val_loss: 0.6684 - val_accuracy: 0.6093\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 55s 41s/step - loss: 0.9813 - accuracy: 0.4688 - val_loss: 0.6275 - val_accuracy: 0.5981\n",
            "Train loss:  0.6396046876907349\n",
            "Test accuracy:  0.579045832157135\n",
            "\n",
            "============FOLD==== 2\n",
            "Epoch 1/2\n",
            "2/2 [==============================] - 57s 42s/step - loss: 0.9005 - accuracy: 0.5000 - val_loss: 0.6732 - val_accuracy: 0.5674\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 56s 42s/step - loss: 0.8228 - accuracy: 0.4531 - val_loss: 0.6674 - val_accuracy: 0.5880\n",
            "Train loss:  0.6639807820320129\n",
            "Test accuracy:  0.5953270792961121\n",
            "\n",
            " Train Log Logg Validation =  0.6378442079869292\n",
            " Test Log Loss Validation =  0.6493596159501152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-0z6-ozIpTx"
      },
      "source": [
        "submission = pd.DataFrame()\n",
        "submission['id'] = test['id']\n",
        "submission['is_iceberg'] = preds\n",
        "submission.to_csv('/content/submission.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}